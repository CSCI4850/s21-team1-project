{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Leabra\\\\}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Matthew Schroder}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle State Tennessee University}\\\\\n",
    "Murfreesboro, TN USA \\\\\n",
    "mrs8n@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Justin Cao}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle State Tennessee University}\\\\\n",
    "Murfreesboro, TN USA \\\\\n",
    "jmc2ei@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} Bradley Carter}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle State Tennessee University}\\\\\n",
    "Murfreesboro, TN USA \\\\\n",
    "bec3x@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{4\\textsuperscript{th} Sean Gately}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle State Tennessee University}\\\\\n",
    "Murfreesboro, TN USA \\\\\n",
    "scg3q@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{5\\textsuperscript{th} Christopher Sutton}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle State Tennessee University}\\\\\n",
    "Murfreesboro, TN USA \\\\\n",
    "cs6t@mtmail.mtsu.edu}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "The objective of our purposed neural networks is to develop a methodology to model and evaluate the accuracy and performance of a particular exercise, in this case the Squat. With our ultimate end game goal being able to live feed forward images using external hardware such as a webcam or a camera. To achieve this/these goal(s), our neural network will be using a convolutional network approach as this ideal with image data input and classification and prediction problems. At this point our input data set will be acquired thought the use of an image scrapper in conjunction with images from google images. After acquiring our images they will be reformatted to numpy arrays before being passed to our neural network. Key layers of our convolutional networks and hyper parameters to note are image sizes of 150 by 150, two conv2d layers of 64 and 128, dropout rates of .7 and .6. Batch sizes of 32 and epochs of 50. These layers and parameters allowed ran more consistently and kept the network and images simple enough to prevent too much overfitting. Results included categorical accuracy in the high nineties even as high as 98 percent with validation also in the high eighties. All while maintaining losses at low percent of single digit or low teens percentage.\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "convolution, physical, exercise, motion, performance, form, technique, therapy\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "\n",
    "The use of neural networks and machine learning  in conjunction with human motions is hardly a new application. Neural networks have been seen to go hand in hand with human motion applications such as playing key roles in physical activity for performance, efficiency and also in healing and rehabilitation such as physical therapy. \n",
    "\n",
    "The objective of our research is to develop a neural network and a methodology for modeling  and evaluating certain physical activity and exercising. For the purpose of our initial research we will be focusing on the squat exercise in particular. \n",
    "\n",
    "The squat is a very popular common compound exercise with experienced and new gym goers alike. That focuses on many key muscles and that is often done with the use of additional weights, and with many variances in stance and form. Hence why when not done correctly or with proper technique an individual can be prone to numerous short and long term injuries. Thus we will be focusing on the squatters form and technique.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "The purposed neural network's ultimate goal is to take live images from external hardware such as a camera or webcam and feed forward that data into our network to tell how \"proper\" the subject's squat technique and form is.\n",
    "\n",
    "To achieve this goal our team choose to use a convolutional network. As our intended input was to be feed forward images inputs this was an easy choice. Convolutional Networks excel for image data inputs, and classification predictions problems. \\cite{b1} They are also designed to be invariant to object position and distortion in the image scene which in dealing with form and technique in physical exercise is ideal.\\cite{b2}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\n",
    "In this section we will go over our baseline methods how we navigate and train our neural network to achieve our desired end game goal. But it is important to note first and foremost we wanted to achieve a neural network that could at the very least determined whether our image input was a squat at all, I.e. a squat detection network. \n",
    "\\subsection{Data Set}\n",
    "To begin we first needed to decide on a means of an initial data set of inputs. We chose to implement an image scrapper to go conjunction with Google images to achieve this. Using this method, we amassed roughly a total of 1200 images give or take of images of physical activities such as walking, running, benching etc. which were all considered \"non\" squats at this point. Alongside roughly 300 actual squat images. \n",
    "\n",
    "Key points to note, our pool of images was not exclusively grayscale or reformatted to be, this was mainly to not bottleneck or limit our available images in anyway as we were concerned about quantity of images available from our image scrapper and google. Our data set of images were however resized to a 150 by 150 pixel scale. Also, it is important to note that our data set of images truly needed to be shuffled or randomized. While this is a general rule with neural networks in general to prevent bias or predictable learning, it was particularly important or very apparent with our data set size. \\cite{b3}. And our images lastly needed to be reshaped and formatted to a NumPy array before being passed to our network.\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{images1.png}}\n",
    "\\caption{Examples of our image data set.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "\n",
    "\\subsection{Convolutional Network}\n",
    "Moving onwards to our choice of Convolutional Neural Network and it's architecture. To recapitulate on why we choose a convolutional network briefly, it is the go to for when dealing with image sets and classification problems. Convolutional Neural Networks preserve the spatial relationship of pixels of said image, features, and weights are learned and reused a cross all pixels and layers. This reusability is key. \\cite{b5}.\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{cnn1.png}}\n",
    "\\caption{Model Summary}\n",
    "\\label{fig1}\n",
    "\\end{figure}\n",
    "\n",
    "Now to begin breaking down our convolutional neural network's architecture. Following from the above figure. We are using two convolutional layers of size 64 and 128 with kernel sizes of 6 by 6 and 4 by 4 respectively. The purpose of these layers is to begin capturing high level features of our input such as edges and such\\cite{b5}.\n",
    "\n",
    "We then implement a batch normalization layer. The purpose and benefit of this layer is to not only standardize our inputs but to also to accelerate the training of our network\\cite{b4} \\cite{b7}.\n",
    "\n",
    "Following along is our pooling layer. Also called subsampling or down sampling\\cite{b6}. This layer works in hand with the previous cov2d layers to dimensional reduce spatial size of previous features and weights while retaining the most important features/weights\\cite{b5}. Essentially a filtering layer.\n",
    "\n",
    "The two dropout layers that are coming next serve are a very precisely simple means to help prevent our network from overfitting, and tuning the inductive bias. \\cite{b7}.\n",
    "\n",
    "Lastly are our dense layers of our Convolutional Neural Networks. Within these layers and Keras are we it handles our output layer. The softmax activation makes our output sum to 1 so we can interpret it as possibilities\\cite{b8}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "In this section includes our most recent implementation and training of our network's output. Hyper parameters to note that we used include. Convolutions layers of 64 6,6 and 128 4,4. Momentum of .50 within the batch normalization layer. Dropout layers of .6 and .7.\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{32-50.png}}\n",
    "\\caption{Batch Size 32 and Epochs of 50.}\n",
    "\\label{fig2}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{50-100.png}}\n",
    "\\caption{Batch Size 50 and Epochs of 100.}\n",
    "\\label{fig3}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{50-20.png}}\n",
    "\\caption{Batch Size 50 and Epochs of 20.}\n",
    "\\label{fig4}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{100-100.png}}\n",
    "\\caption{Batch Size 100 and Epochs of 100.}\n",
    "\\label{fig5}\n",
    "\\end{figure}\n",
    "\n",
    "As you can see our initial figure shows a wild amount of loss compared to the others."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "Here we are restating our goals and progress. To that end, we believe we achieved a neural network that at the very least is learned and trained to be a squat classification network. To further progress our neural network, we will be looking to minimize our losses even further. Future steps would be to implement methods to measure the performance or correctness of said squat input. One potential avenue would be to identify individual mistakes in the performance of a squat such as arching the spine, inward collapse of the knees, leaning forward, and standing on toes. Crafting a dataset of categorical images of each mistake alongside correctly squats. Providing a variety of orientations of the camera and individual, with a variety of alterations to clothing, hair, and body. But, it would be reasonable to use camera positions that make sense for the networks use cases."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
