Roles
----------
Collecting data - Brad
Processing data - Chris
Project Mgmt - Matt 
Network training - Sean and Justin

Mar 12
-------------------
Struggled with how to start, opted to scrape first for a faster start

Setup the general roles for each memeber,  

Having two datasets for testing and training
	* Building or using pre existing scraper
	* Processing scraped images
	* Repeat for various kinds of images

Having basic architecture together
	* Research options for architecture
	* Test options' performances

Mar 30
-------------------
Brad scrapped images with script and we divided the job of classifying them up.

Apr 1
-------------------
Created some functions to read in the good/bad images as numpy arrays from
their respective directories. 
Were using the directory name as the classfier. 

Apr 16 
-------------------
Randomized numpy array with np.random.shuffle. Improved performance by an
average of 10%. 

Brad attempting 3 diff feature detectors each in a block. One for knees.
1715 images as of right now. Almost 1200 are of squats -> get more non squats

Cleaned up images and narrowed scope down to categorizing based on mostly side
views of a squat

Apr 19
-------------------
custom function to read in a process data was to complicated for a team
structure switched to using image generator classto read in and manipulate
images because it was simpler. Converted to a numpy array after because it was more explicit.

custom prediction method at the bottom shows the networks guess (left) and the
actual classification (right)

so far the simpler network architectures do a better job of not overfitting (1-2 smallish conv2d layers)

batch normalization improves overfitting issue dramatically, a momentum of 0.4 to 0.6 seems to be the sweet spot
https://keras.io/api/layers/normalization_layers/batch_normalization

starting to place performance graphs as a sort of log. 

